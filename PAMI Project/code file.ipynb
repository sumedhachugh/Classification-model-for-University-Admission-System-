{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.externals.six import StringIO\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>380</td>\n",
       "      <td>3.61</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>660</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>800</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>640</td>\n",
       "      <td>3.19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "      <td>2.93</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   admit  gre   gpa  rank\n",
       "0      0  380  3.61     3\n",
       "1      1  660  3.67     3\n",
       "2      1  800  4.00     1\n",
       "3      1  640  3.19     4\n",
       "4      0  520  2.93     4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv(\"/Users/sumedha/Desktop/Decision Tree.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing the data\n",
    "X = df.iloc[:, 1:].values\n",
    "y = df.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.36, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting decision tree\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five test set values are [[580.     3.4    2.  ]\n",
      " [440.     2.98   3.  ]\n",
      " [560.     2.65   3.  ]\n",
      " [660.     3.07   3.  ]\n",
      " [680.     3.34   2.  ]\n",
      " [620.     3.18   2.  ]]\n",
      "Predictions for first five test set values are [0 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "y_predict_train = clf.predict(X_train)\n",
    "y_predict_test = clf.predict(X_test)\n",
    "print('First five test set values are {}'.format(X_test[:6,:]))\n",
    "print('Predictions for first five test set values are {}'.format(y_predict_test[:6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on test data is 0.61 out of 1\n",
      "The accuracy of the classifier on training data is 0.99 out of 1\n",
      "The f1_score of the classifier on test data is 0.40 out of 1\n",
      "The f1_score of the classifier on training data is 0.99 out of 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "acc_test = accuracy_score(y_test,y_predict_test)\n",
    "acc_train = accuracy_score(y_train,y_predict_train)\n",
    "f1_test = f1_score(y_test,y_predict_test)\n",
    "f1_train = f1_score(y_train,y_predict_train)\n",
    "print('The accuracy of the classifier on test data is {:.2f} out of 1'.format(acc_test))\n",
    "print('The accuracy of the classifier on training data is {:.2f} out of 1'.format(acc_train))\n",
    "print('The f1_score of the classifier on test data is {:.2f} out of 1'.format(f1_test))\n",
    "print('The f1_score of the classifier on training data is {:.2f} out of 1'.format(f1_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[69, 30],\n",
       "       [26, 19]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_predict_test)\n",
    "np.set_printoptions(precision=2)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "True_pos = cnf_matrix[0][0]\n",
    "False_neg = cnf_matrix[0][1]\n",
    "False_pos = cnf_matrix[1][0]\n",
    "True_neg = cnf_matrix[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier is 0.61 out of 1\n",
      "The Recall_Accepted of the classifier is 0.70 out of 1\n",
      "The  Recall_score Rejected of the classifier is 0.42 out of 1\n",
      "The  Precision_score Accepted of the classifier is 0.73 out of 1\n",
      "The  Precision_score Rejected of the classifier is 0.39 out of 1\n",
      "The  F1_score Accepted of the classifier is 0.71 out of 1\n",
      "The  F1_score Rejected of the classifier is 0.40 out of 1\n"
     ]
    }
   ],
   "source": [
    "Accuracy_Acc = (True_pos+True_neg)/(True_pos+False_pos+False_neg+True_neg)\n",
    "R_Accepted = (True_pos)/(True_pos+False_neg)\n",
    "R_Rejected = (True_neg)/(True_neg+False_pos)\n",
    "P_Accepted = (True_pos)/(True_pos+False_pos)\n",
    "P_Rejected = (True_neg)/(True_neg+False_neg)\n",
    "F1_Accepted = (2*R_Accepted*P_Accepted)/(R_Accepted+P_Accepted)\n",
    "F1_Rejected = (2*R_Rejected*P_Rejected)/(R_Rejected+P_Rejected)\n",
    "print('The accuracy of the classifier is {:.2f} out of 1'.format(Accuracy_Acc))\n",
    "print('The Recall_Accepted of the classifier is {:.2f} out of 1'.format(R_Accepted))\n",
    "print('The  Recall_score Rejected of the classifier is {:.2f} out of 1'.format(R_Rejected))\n",
    "print('The  Precision_score Accepted of the classifier is {:.2f} out of 1'.format(P_Accepted))\n",
    "print('The  Precision_score Rejected of the classifier is {:.2f} out of 1'.format(P_Rejected))\n",
    "print('The  F1_score Accepted of the classifier is {:.2f} out of 1'.format(F1_Accepted))\n",
    "print('The  F1_score Rejected of the classifier is {:.2f} out of 1'.format(F1_Rejected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving Accuracy using Hyperparameters\n",
    "model = tree.DecisionTreeClassifier(max_depth = 5, min_samples_leaf=11,min_samples_split=35)\n",
    "model.fit(X_train, y_train)\n",
    "y_predict_train_m = model.predict(X_train)\n",
    "y_predict_test_m = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on test data is 0.71 out of 1\n",
      "The accuracy of the classifier on training data is 0.75 out of 1\n",
      "The f1_score of the classifier on test data is 0.46 out of 1\n",
      "The f1_score of the classifier on training data is 0.51 out of 1\n"
     ]
    }
   ],
   "source": [
    "acc_test = accuracy_score(y_test,y_predict_test_m)\n",
    "acc_train = accuracy_score(y_train,y_predict_train_m)\n",
    "f1_test = f1_score(y_test,y_predict_test_m)\n",
    "f1_train = f1_score(y_train,y_predict_train_m)\n",
    "print('The accuracy of the classifier on test data is {:.2f} out of 1'.format(acc_test))\n",
    "print('The accuracy of the classifier on training data is {:.2f} out of 1'.format(acc_train))\n",
    "print('The f1_score of the classifier on test data is {:.2f} out of 1'.format(f1_test))\n",
    "print('The f1_score of the classifier on training data is {:.2f} out of 1'.format(f1_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_predict_test_m)\n",
    "np.set_printoptions(precision=2)\n",
    "True_pos = cnf_matrix[0][0]\n",
    "False_neg = cnf_matrix[0][1]\n",
    "False_pos = cnf_matrix[1][0]\n",
    "True_neg = cnf_matrix[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier is 0.71 out of 1\n",
      "The Recall_Accepted of the classifier is 0.85 out of 1\n",
      "The  Recall_score Rejected of the classifier is 0.40 out of 1\n",
      "The  Precision_score Accepted of the classifier is 0.76 out of 1\n",
      "The  Precision_score Rejectted of the classifier is 0.55 out of 1\n",
      "The  F1_score Accepted of the classifier is 0.80 out of 1\n",
      "The  F1_score Rejected of the classifier is 0.46 out of 1\n"
     ]
    }
   ],
   "source": [
    "Accuracy_Acc = (True_pos+True_neg)/(True_pos+False_pos+False_neg+True_neg)\n",
    "R_Accepted = (True_pos)/(True_pos+False_neg)\n",
    "R_Rejected = (True_neg)/(True_neg+False_pos)\n",
    "P_Accepted = (True_pos)/(True_pos+False_pos)\n",
    "P_Rejected = (True_neg)/(True_neg+False_neg)\n",
    "F1_Accepted = (2*R_Accepted*P_Accepted)/(R_Accepted+P_Accepted)\n",
    "F1_Rejected = (2*R_Rejected*P_Rejected)/(R_Rejected+P_Rejected)\n",
    "print('The accuracy of the classifier is {:.2f} out of 1'.format(Accuracy_Acc))\n",
    "print('The Recall_Accepted of the classifier is {:.2f} out of 1'.format(R_Accepted))\n",
    "print('The  Recall_score Rejected of the classifier is {:.2f} out of 1'.format(R_Rejected))\n",
    "print('The  Precision_score Accepted of the classifier is {:.2f} out of 1'.format(P_Accepted))\n",
    "print('The  Precision_score Rejectted of the classifier is {:.2f} out of 1'.format(P_Rejected))\n",
    "print('The  F1_score Accepted of the classifier is {:.2f} out of 1'.format(F1_Accepted))\n",
    "print('The  F1_score Rejected of the classifier is {:.2f} out of 1'.format(F1_Rejected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn=['gpa','gre','rank']\n",
    "cn=['0', '1']\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(model,\n",
    "                     out_file = dot_data,\n",
    "                     feature_names=fn,\n",
    "                     class_names= cn,\n",
    "                     filled=True, rounded=True,impurity = False)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It creates a pdf and stores it in working folder\n",
    "graph.write_pdf(\"Tree_Visualization.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
